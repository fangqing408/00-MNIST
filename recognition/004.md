# 反向传播

## 1 >> 构建 one_hot 向量

one_hot 的向量的定义为，标签的位置为 1，其余位置为 0，也可以把他理解为，真实值向量，矩阵的大小和维度一定要心里有数，否则进行矩阵乘法运算的时候，没办法对应好。

```
def backward(image, activated1, pooled1, pool_indices1, activated2, pooled2, pool_indices2, flattened, predictions,
             label, conv_kernel1, conv_kernel2, fc_weights, fc_bias):
    one_hot = np.zeros((10,))
    one_hot[label] = 1
    y_true = one_hot
    d_fc_output = predictions - y_true
    ...
```

## 2 >> 全连接层反向传播

格局前向传播的公式，第 n 个神经元的第 m 个参数的梯度为：

$\frac{\partial J}{\partial w_{nm}}=\frac{1}{N}\sum_{i=1}^N(\hat y_n^{(i)}-y_n^{(i)})x_m^{(i)}\qquad \frac{\partial J}{\partial b_n}=\frac{1}{N}\sum_{i=1}^N(\hat y_n^{(i)}-y_n^{(i)})$

因为我们是单张图片，没有涉及到批次的概念，上面式子的求平均的操作可以省略掉，对应的代码的编写方式如下：

```
    ...
    d_fc_weights = np.outer(flattened, d_fc_output)
    d_fc_bias = d_fc_output
    d_flattened = np.dot(d_fc_output, fc_weights.T)
    ...
```

## 3 >> 隐藏层的反向传播

- 001 >> 池化层反向传播

最后一次池化后面衔接的就是全连接层，Flatten 层的反向传播就是前向传播的逆操作。因为展平操作是线性且无参数的，它的梯度就是形状的还原。

```
    ...
    d_pooled2 = d_flattened.reshape(pooled2.shape)
    d_activated2 = np.zeros_like(activated2)
    ...
```

- 002 >> 激活层反向传播

相较于前面池化层的反向传播更加好理解，因为激活之后紧跟着的就算池化操作，激活操作的输出是池化操作的输入，不是最大值的位置没有参与前向传播梯度为 0，最大值的位置的梯度就是池化层传回来的梯度，不重叠的池化操作的话，+= 可以写为 =。

```
    ...
    for c in range(pooled2.shape[-1]):
        for i in range(pooled2.shape[0]):
            for j in range(pooled2.shape[1]):
                idx = tuple(pool_indices2[i, j, c])
                if 0 <= idx[0] < activated2.shape[0] and 0 <= idx[1] < activated2.shape[1]:
                    d_activated2[idx[0], idx[1], c] += d_pooled2[i, j, c]
    ...
```

- 003 >> 卷积层反向传播

更加的好理解，因为卷积层到激活层经过了 ReLU，操作，根据导数的链式法则，正数即是激活层传过来的梯度，负数为 0。

```
    ...
    d_convolved2 = d_activated2 * (activated2 > 0).astype(float)
    ...
```

## 4 >> 卷积核参数反向传播

反向传播较难理解的一部分，首先，先构造出来全 0 的 d_kernel2 矩阵方便累加，因为前向传播对特征图进行了 padding，这里我们也要进行 padding。

```
    ...
    d_kernel2 = np.zeros_like(conv_kernel2)
    padded_pooled1 = np.pad(pooled1, ((1, 1), (1, 1), (0, 0)), mode='constant', constant_values=0)
    ...
```

卷积核的输出通道数就是卷积核的个数，卷积核的输入通道数就是原特征图的通道数，所以可以简单的将卷积核理解为一个四维的矩阵，现在单独考虑其特殊的三维情况（输出的通道数为 1）。

在这个情况下，卷积核的每个每个位置的梯度更好理解一点，最简单的代码的写法如下，每个位置的梯度，按照前向传播的对应写法来累加，避免错误的产生。

```
    ...
    for c_out in range(d_convolved2.shape[-1]):
        for c_in in range(padded_pooled1.shape[-1]):
            for i in range(d_convolved2.shape[0]):
                for j in range(d_convolved2.shape[1]):
                    d_kernel2[:, :, c_in, c_out] += d_convolved2[i, j, c_out] * padded_pooled1[i:i + 3, j:j + 3, c_in]
    ...
```

第一层的卷积的反向传播过程和上面介绍的一致，下面就不在赘述了。

## 5 >> 构建步骤

- 005 >> [模型训练](https://github.com/fangqing408/00-MNIST/blob/master/recognition/005.md)
- 006 >> [模型保存和预测](https://github.com/fangqing408/00-MNIST/blob/master/recognition/006.md)
