# 网络参数初始化

## 1 >> He 初始化

作者初始化了两个卷积层和一个全连接层的权重和偏置，偏置设置为 0，卷积层和全连接层按照 He 初始化设置，适用于激活函数为 ReLU 的网络。

```
def init():
    fan_in_conv1 = 3 * 3 * 1 # 输入通道数乘以卷积核的维度，参数的数量
    std_dev_conv1 = np.sqrt(2.0 / fan_in_conv1) # He 初始化关键，计算标准差
    conv_kernel1 = np.random.randn(3, 3, 1, 16) * std_dev_conv1
    # 初始化第一个卷积核参数为随机值乘以标准差，输入通道为 1，输出通道为 16
    fan_in_conv2 = 3 * 3 * 16
    std_dev_conv2 = np.sqrt(2.0 / fan_in_conv2)
    conv_kernel2 = np.random.randn(3, 3, 16, 32) * std_dev_conv2
    fc_input_size = (28 // 4) ** 2 * 32 # 全连接层的大小
    fan_in_fc = fc_input_size
    std_dev_fc = np.sqrt(2.0 / fan_in_fc) # 计算标准差
    fc_weights = np.random.randn(fc_input_size, 10) * std_dev_fc # 初始化全连接层
    fc_bias = np.zeros(10) # 初始化偏置为 0
    return conv_kernel1, conv_kernel2, fc_weights, fc_bias
```
