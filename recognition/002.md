# 网络参数初始化

## 1 >> He 初始化

作者初始化了两个卷积层和一个全连接层的权重和偏置，偏置设置为 0，卷积层和全连接层按照 He 初始化设置，适用于激活函数为 ReLU 的网络。

- 001 >> He 初始化理论实现

因为 ReLU 激活函数会将所有负值清零，导致输出的有效激活数量减半，假设输入均匀分布，均值为 0，方差为 $\sigma^2$，输出的方差会减半为 $\frac{1}{2}\sigma^2$，正向传播要保证层与层之间的方差一致，避免梯度消失或者爆炸，假设输入 x 的均值为 0，方差为 $\sigma_x^2$，权重 W 的均值为 0，方差为 Var(W)。

输出方差 Var(y) = Var(Wx) = Var(W) $\cdot$ Var(x) $\cdot n_{in}$，其中的 $n_{in}$ 是输入维度 fan_in，由于 ReLU 使得方差减半，实际输出的方差为 Var(y) = $\frac{1}{2} \cdot$ Var(W) $\cdot n_{in} \cdot \sigma_x^2$。

先要实现 Var(y) = $\sigma_x^2$，Var(W) = $\frac{2}{n_{in}}$。

求得标准差为 $\sqrt{\frac{2}{fan_in}}$

```
def init():
    fan_in_conv1 = 3 * 3 * 1 # 输入通道数（卷积核的个数）乘以卷积核的大小，参数的数量
    std_dev_conv1 = np.sqrt(2.0 / fan_in_conv1) # He 初始化关键，计算标准差
    conv_kernel1 = np.random.randn(3, 3, 1, 16) * std_dev_conv1
    # 初始化第一个卷积核参数为随机值乘以标准差，输入通道为 1，输出通道为 16
    fan_in_conv2 = 3 * 3 * 16
    std_dev_conv2 = np.sqrt(2.0 / fan_in_conv2)
    conv_kernel2 = np.random.randn(3, 3, 16, 32) * std_dev_conv2
    fc_input_size = (28 // 4) ** 2 * 32 # 全连接层的大小
    fan_in_fc = fc_input_size
    std_dev_fc = np.sqrt(2.0 / fan_in_fc) # 计算标准差
    fc_weights = np.random.randn(fc_input_size, 10) * std_dev_fc # 初始化全连接层
    fc_bias = np.zeros(10) # 初始化偏置为 0
    return conv_kernel1, conv_kernel2, fc_weights, fc_bias
```

## 2 >> 构建步骤

- 003 >> [前向传播](https://github.com/fangqing408/00-MNIST/blob/master/recognition/003.md)
- 004 >> [反向传播](https://github.com/fangqing408/00-MNIST/blob/master/recognition/004.md)
- 005 >> [模型训练](https://github.com/fangqing408/00-MNIST/blob/master/recognition/005.md)
- 006 >> [模型保存和预测](https://github.com/fangqing408/00-MNIST/blob/master/recognition/006.md)
